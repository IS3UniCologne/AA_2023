{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "\n",
    "In this workshop we provide a very short introduction to neural networks in Python. This is very far from a comprehensive coverage of the topic but can provide a quick start for those who wish to learn more about the topic in their own time. We will cover a classification and a regression taks using `keras` as our python package of choice. If you want to try and implement a NN from scratch there are several good online tutorials that can help you do so (see [here](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) for example). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological inspiration\n",
    "The (for our purpose) smallest stand-alone element in the human brain is the neuron. Its understanding and computational recreation build the foundation for ANNs. A simplified image of a \"real\" neuron can be seen below\n",
    "\n",
    "![](bio_neuron.png)\n",
    "\n",
    "Dendrites are connecting to the axons (or \"outputs\") of other neurons, for instance nerves in the sensory system or other processing neurons. In the nucleus, these input signals are aggregated and forwarded through the axon. The axon terminals then connect to further neurons to build the neural network. The connection between axon terminal and dendrite is what we are calling a synapse. In the human brain, there are billions of neurons and $10^{14} - 10^{15}$ synapses in the human brain. If each synapse (or more precisely, its connection strength) would be represented by 8 bits or one byte, just storing these numbers would take 1000 TB already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational implementation\n",
    "To recreate neural networks artificially, neurons have to be defined. The common mathematical model used for this purpose is depicted below.\n",
    "\n",
    "![](math_neuron.jpeg)\n",
    "\n",
    "From a certain number of input synapses $x_i$, signals come in with a weight factor of $w_i$. This represents the strength of the synapse. In the _nuclues_ these weighted inputs are aggregated and a bias is added. (The bias is not shown in every model, but it does make the neural network more generalizable). After adding of the weighted inputs and the bias, everything is fed into a (non-linear) activation function. The output is then either fed forward to further neurons or is the output of your neural network. If there is only one neuron that takes direct inputs and whose output is your interest, the model is called a single-layer perceptron. Many of these neurons can create almost arbitrary logical connections and functions, making ANNs very powerful. In this case, we are talking about a multi-layer perceptron (MLP) model. \n",
    "\n",
    "![](mlp-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "The activation function is (to some degree) the hear of the neural network. Without a non-linear activation function, all hidden layers do not add any value, but are instead a complicated way to represent a liner model. Only with a non-linear activation function, ANNs can recreate non-linear hypothesis functions. In the beginning of research on the ANNs in the scope of AI, typically a unit step was used as activation function. The unit step is $0$ for inputs smaller than $0$ and $1$ otherwise. The idea behind this is to recreate the behavior of a biological neuron that _fires_ if a certain threshold of inputs is exceeded. Today, other activation functions are more typically used. This is linked to better mathematical qualities in terms of learning behavior and convergence. Some of the most popular activation functions are:\n",
    "\n",
    "Sigmoid: $\\sigma(z) = \\frac{1}{1+exp(-z)}$\n",
    "\n",
    "Hyperbolic tangent: $\\sigma(z) = \\frac{2}{1+exp(-2z)} -1 $\n",
    "\n",
    "ReLU (Rectified Linear Unit): $\\sigma(z) = z\\quad  for\\ z>0,\\ 0\\ otherwise$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "As learning of ANNs is a non-trivial mathematical task, we are only aiming for an intuitive understanding here. Let's have a look at our complete MLP first.\n",
    "\n",
    "The general learning tasks consists of two steps, which are repeated until the algorithm converges:\n",
    "1. __Feedforward: Calculating the predicted output ŷ and the associated loss__. At first, we randomly assign values for the weights (and the biases). Based on the input features, the output value is calculated.\n",
    "2. __Backpropagation: Updating the weights W and biases b__. If the output value and the target value differ, the weights and biases are updated. To do this, it is calculated how much each weight and bias contributes to the error. Proportionally to this, they are then corrected (scaled with a small learning factor). In this sense, the updating rule has some similarity to gradient descent, only that is is propagated through the entire network, which is why this algorithm is called backpropagation.\n",
    "\n",
    "The training routine for a simple 2-layered MLP is shown in the below figure:\n",
    "\n",
    "![](training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "The main hyperparameters of an MLP are: \n",
    "\n",
    "1. Number of hidden layers\n",
    "1. Number of nodes\n",
    "4. Activation function\n",
    "\n",
    "The number of hidden layers and number of nodes (its activation function could be understood as a hyperparameter, but that is typically not done). The more layers and nodes there are (and the denser the network is, i.e. the more edges have a non-zero weight) the harder it gets to learn the model. That's the reason why bigger ANNs are normally not trained on a local computer anymore, but on specialized computers. Furthermore, there are additional libraries for python to improve the efficiency of ANNs, e.g. TensorFlow or Keras, which we take a first look at in today's tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` is one of the most popular Deep Learning libraries. `PyTorch`, `PyTorch` and `Jax` are the most used numerical platforms in Python to build Deep Learning algorithms but they can be quite complex and difficult to use.\n",
    "\n",
    "Keras, by contrast is easy to use and is capable of running on top of multiple low-level tensor operation frameworks. The full documentation of the keras API can be found [here](https://keras.io).\n",
    "\n",
    "Note that `scikit learn` also features an MLP implementation (see [here](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)). Yet, `keras` has advanced to be one of the most popular frameworks used in practice, which is why we focus on it in this short tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `Keras` to command `PyTorch`, therefore we need to install both.\n",
    "PyTorch's installation method varies by platform and environment manager, all of the options are listed here: https://pytorch.org/get-started/locally/\n",
    "\n",
    "If you use the provided `environment.yml` specification, you can just recreate your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for classification in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay with our example, we will build a NN that predicts the class of a breast cancer by categorizing it as either malignant or begnign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# supress versioning warnings of keras\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Because keras can work with multiple different backends, it is important to specify that we want to use PyTorch before importing keras for the first time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential # sequential model: https://keras.io/guides/sequential_model/\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.6</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.8</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                       \n",
       "842302         M        17.99         10.38           122.8     1001.0   \n",
       "842517         M        20.57         17.77           132.9     1326.0   \n",
       "\n",
       "        smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                          \n",
       "842302          0.11840           0.27760          0.3001   \n",
       "842517          0.08474           0.07864          0.0869   \n",
       "\n",
       "        concave points_mean  symmetry_mean  ...  radius_worst  texture_worst  \\\n",
       "id                                          ...                                \n",
       "842302              0.14710         0.2419  ...         25.38          17.33   \n",
       "842517              0.07017         0.1812  ...         24.99          23.41   \n",
       "\n",
       "        perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "id                                                                         \n",
       "842302            184.6      2019.0            0.1622             0.6656   \n",
       "842517            158.8      1956.0            0.1238             0.1866   \n",
       "\n",
       "        concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "id                                                              \n",
       "842302           0.7119                0.2654          0.4601   \n",
       "842517           0.2416                0.1860          0.2750   \n",
       "\n",
       "        fractal_dimension_worst  \n",
       "id                               \n",
       "842302                  0.11890  \n",
       "842517                  0.08902  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "cancer_df = pd.read_csv(\"../data/breast_cancer.csv\", index_col = \"id\")\n",
    "cancer_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x and Y\n",
    "X = cancer_df.iloc[:,1:31] # include full feature vector\n",
    "y = cancer_df[\"diagnosis\"]\n",
    "\n",
    "\n",
    "# encode categorical target verctor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initializing and Training the ANN__\n",
    "\n",
    "We start by defining the type of model we want to build. There are two types of models available in Keras: the [Sequential model](https://keras.io/models/sequential/) and the Model class used with [functional API](https://keras.io/models/model/). Then we simply add the input-, 2 hidden- and output-layers.\n",
    "\n",
    "Between them, we are using [dropout](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) to prevent overfitting (dropout rate should be between 20% and 50%).\n",
    "\n",
    "![](dropout.png)\n",
    "\n",
    "At every layer, we use “Dense” which means that the nodes are fully connected.\n",
    "\n",
    "The input-layer takes 30 inputs (because our feature vector includes 30 features) as input and outputs it with a shape of 16, which is the number of nodes in the first hidden layer that we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pass the following parameters:\n",
    "\n",
    "- input_shape - number of columns of the dataset (only for input layer)\n",
    "\n",
    "- units - number of neurons and dimensionality of outputs to be fed to the next layer, if any\n",
    "\n",
    "- activation - activation function which is ReLU in this case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the input layer and the first hidden layer (with 30 nodes)\n",
    "classifier.add(Dense(input_shape = (30,), \n",
    "                     units=30,          #dimensionality of the output space (#nodes in the first hidden layer)\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional second layer, also with 15 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units= 15,\n",
    "                     activation='relu'))\n",
    "\n",
    "# Adding dropout to prevent overfitting\n",
    "classifier.add(Dropout(rate=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the output layer. Since we perform a binary classification, a single output node suffices. We use a sigmoidal activation function for this last node which is often used when dealing with binary classfication problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units= 1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compile the model to configure it for training. We add the following parameters:\n",
    "- `optimizer`: Here we use the adam optimizer, an optimizer with higher performance in many cases than stochastic gradient descent (SGD). See [here](https://keras.io/optimizers/) for a list of all optimzers implemented in `keras`.\n",
    "- `loss`: specifies the loss to be minimized. In this example we use binary crossentropy, a common loss for binary classification tasks. See [here](https://keras.io/losses/) for an overview of available losses in keras \n",
    "- `metrics`:  metric function is similar to a loss function, except that the results from evaluating a metric are not used when training the model and merely function as indicator of model performance to the data scientist. An overview ov available metrics can be found [here](https://keras.io/metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "classifier.compile(optimizer=\"adam\",    # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments\n",
    "              loss=\"binary_crossentropy\",  # this is a good loss for binary classification\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">465</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                │        \u001b[38;5;34m930\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                │        \u001b[38;5;34m465\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                │          \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │         \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,411</span> (5.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,411\u001b[0m (5.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,411</span> (5.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,411\u001b[0m (5.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to train our model. We do this with a batch_size of 50 and for 100 epochs.\n",
    "\n",
    "- `batch_size` defines the number of samples that will be propagated through the network \n",
    "- `epoch` defines the number of iteration over the entire training data\n",
    "\n",
    "In general a larger batch-size results in faster training, but does not always converge fast. A smaller batch-size is slower in training but it can converge faster. This is definitely problem dependent and you need to try out a few different values (the standard batch-size is 32). The same goes for the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6133 - loss: 0.6507 \n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7590 - loss: 0.5468\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8732 - loss: 0.4202\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8613 - loss: 0.3760\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8998 - loss: 0.3321\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9289 - loss: 0.2880\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9165 - loss: 0.2550\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.2283\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9323 - loss: 0.2098\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9400 - loss: 0.1942\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9431 - loss: 0.1850\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9527 - loss: 0.1960\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9490 - loss: 0.1593\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9796 - loss: 0.1230\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.1248\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1289\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9521 - loss: 0.1207\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9617 - loss: 0.1029\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9574 - loss: 0.1246\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.1328\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9591 - loss: 0.1135\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9708 - loss: 0.0922\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.0986 \n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.0892\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.0894\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9585 - loss: 0.0976\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0742\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9806 - loss: 0.0748\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.0799\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9763 - loss: 0.0877\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.1224\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9803 - loss: 0.0810\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0798\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0724\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9725 - loss: 0.0877\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0607\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9817 - loss: 0.0712\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9743 - loss: 0.0740\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9814 - loss: 0.0645\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9911 - loss: 0.0575\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0942\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9931 - loss: 0.0476\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9807 - loss: 0.0531\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0766\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0572\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9870 - loss: 0.0483\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0701\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0638\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9793 - loss: 0.0554\n",
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0583\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0384\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9848 - loss: 0.0618\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9802 - loss: 0.0673\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0349\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0356\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9889 - loss: 0.0487\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0533\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0428\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9754 - loss: 0.0518\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0675\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0535\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9906 - loss: 0.0442\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0420\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.0657\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9768 - loss: 0.0552\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9954 - loss: 0.0332\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0415\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0390\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9835 - loss: 0.0440\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9913 - loss: 0.0401\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0391\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9902 - loss: 0.0296\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9945 - loss: 0.0363\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9952 - loss: 0.0269\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0386\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0475\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0325\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0366\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9848 - loss: 0.0366\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0427\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9867 - loss: 0.0323\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9933 - loss: 0.0222\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0252\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9918 - loss: 0.0263\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9830 - loss: 0.0435\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9928 - loss: 0.0350\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9945 - loss: 0.0225\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9958 - loss: 0.0223\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0249\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0354\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0417\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9967 - loss: 0.0197\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9942 - loss: 0.0234\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0255\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0201\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9941 - loss: 0.0236\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0326\n",
      "Epoch 98/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9898 - loss: 0.0346\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9936 - loss: 0.0337\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x280ce37f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=50, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Confusion Matrix\n",
      "[[107   1]\n",
      " [  1  62]]\n",
      "\n",
      "Accuracy\n",
      "0.9883\n",
      "\n",
      "Precision\n",
      "0.9841\n"
     ]
    }
   ],
   "source": [
    "# Report classification performance on test set\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "accuracy_score = accuracy_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "precision_score = precision_score(y_test, classifier.predict(X_test).round(decimals=0).astype(int))\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix)\n",
    "print()\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score.round(decimals=4))\n",
    "print()\n",
    "print(\"Precision\")\n",
    "print(precision_score.round(decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks for regression in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks can also be trained for regression tasks. The logic is exactly the same, yet some of the parameters, such as loss, metrics, input and ouput as well as typical activation functions might have to be adapted to the specific case. There are a range of very good tutorial online which we encourage you to take a look at (for example [here](https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/)). \n",
    "\n",
    "We will cover a simple implimentation on the `Diamonds` dataset. The objective in this task is to predict the price of a particular dimond based on different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14237</th>\n",
       "      <td>1.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>5768</td>\n",
       "      <td>6.78</td>\n",
       "      <td>6.72</td>\n",
       "      <td>4.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>0.33</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>579</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.56</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33402</th>\n",
       "      <td>0.28</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>F</td>\n",
       "      <td>IF</td>\n",
       "      <td>62.4</td>\n",
       "      <td>55.0</td>\n",
       "      <td>828</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.17</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17776</th>\n",
       "      <td>1.50</td>\n",
       "      <td>Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>64.2</td>\n",
       "      <td>56.0</td>\n",
       "      <td>7161</td>\n",
       "      <td>7.20</td>\n",
       "      <td>7.26</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12796</th>\n",
       "      <td>1.11</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>H</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.5</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5335</td>\n",
       "      <td>6.61</td>\n",
       "      <td>6.64</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "14237   1.23      Ideal     H     SI1   63.0   55.0   5768  6.78  6.72  4.27\n",
       "7069    0.33  Very Good     E     SI1   59.1   58.0    579  4.51  4.56  2.68\n",
       "33402   0.28      Ideal     F      IF   62.4   55.0    828  4.20  4.17  2.61\n",
       "17776   1.50       Good     H     SI2   64.2   56.0   7161  7.20  7.26  4.64\n",
       "12796   1.11  Very Good     H     SI1   62.5   58.0   5335  6.61  6.64  4.14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>53940.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.797940</td>\n",
       "      <td>61.749405</td>\n",
       "      <td>57.457184</td>\n",
       "      <td>3932.799722</td>\n",
       "      <td>5.731157</td>\n",
       "      <td>5.734526</td>\n",
       "      <td>3.538734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.474011</td>\n",
       "      <td>1.432621</td>\n",
       "      <td>2.234491</td>\n",
       "      <td>3989.439738</td>\n",
       "      <td>1.121761</td>\n",
       "      <td>1.142135</td>\n",
       "      <td>0.705699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>326.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>4.720000</td>\n",
       "      <td>2.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>61.800000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>3.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.040000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>5324.250000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>6.540000</td>\n",
       "      <td>4.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.010000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18823.000000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>58.900000</td>\n",
       "      <td>31.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              carat         depth         table         price             x  \\\n",
       "count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   \n",
       "mean       0.797940     61.749405     57.457184   3932.799722      5.731157   \n",
       "std        0.474011      1.432621      2.234491   3989.439738      1.121761   \n",
       "min        0.200000     43.000000     43.000000    326.000000      0.000000   \n",
       "25%        0.400000     61.000000     56.000000    950.000000      4.710000   \n",
       "50%        0.700000     61.800000     57.000000   2401.000000      5.700000   \n",
       "75%        1.040000     62.500000     59.000000   5324.250000      6.540000   \n",
       "max        5.010000     79.000000     95.000000  18823.000000     10.740000   \n",
       "\n",
       "                  y             z  \n",
       "count  53940.000000  53940.000000  \n",
       "mean       5.734526      3.538734  \n",
       "std        1.142135      0.705699  \n",
       "min        0.000000      0.000000  \n",
       "25%        4.720000      2.910000  \n",
       "50%        5.710000      3.530000  \n",
       "75%        6.540000      4.040000  \n",
       "max       58.900000     31.800000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHPCAYAAAD9FLv9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwj0lEQVR4nO3deZzNdf//8ec5ZjWDYQZfZIwtFGMsFeNSkyX5SlJdWS779G1BG5UUBrky2cLX1WoJlXy5QlHZqZR9KXFRxpolS/Zlxsz794efcznNcMYVn8+887jfbud2M+/zOee8zsG8zuv1fn/eH48xxggAADjC63YAAADcSEi8AAA4iMQLAICDSLwAADiIxAsAgINIvAAAOIjECwCAg0i8AAA4iMQLAICDSLw3mBUrVqhly5aKjY1VaGioihcvrrp166pnz55+x8XFxem+++5zKcorW7JkiTwejzwej95///0cj2nQoIE8Ho/i4uL8xuPi4tSpU6drHlOnTp2yvdb1lpGRoeLFi6tOnTqXPSYrK0uxsbGKj4/P9fNe/HyXLFlyDaIE8Hsk3hvInDlzlJiYqOPHj2vIkCGaN2+eRo0apXr16mnq1Kluh3fVChQooHHjxmUb3759u5YsWaKCBQtmu2/GjBnq27fvNY+lb9++mjFjxjV/3isJDg5W+/bttWLFCm3atCnHYxYsWKDdu3crOTnZ0dgAXB6J9wYyZMgQlS1bVnPnzlXr1q111113qXXr1ho2bJh27drldnhXrVWrVvrmm2/0008/+Y2PHz9epUqVUr169bI9pkaNGipfvvw1j6V8+fKqUaPGNX/eQC4m1PHjx+d4//jx4xUSEqJ27do5GRaAKyDx3kAOHz6smJgYBQUFZbvP6835n8KXX36pmjVrKjw8XJUrV87xF/zGjRvVokULFS5cWGFhYUpISNDEiRN99xtjVLx4cXXr1s03lpmZqcKFC8vr9erAgQO+8REjRigoKEhHjx4N+H4aN26s0qVL+8WUlZWliRMnqmPHjjm+p9+3mrOysjRo0CBVqlRJ4eHhioqKUnx8vEaNGuU75uDBg3rsscdUunRphYaGqmjRoqpXr54WLFjgOyanVrPH41H37t01efJkValSRfnz51f16tU1e/bsbHHNmjVL8fHxCg0NVbly5TRq1Cj1799fHo/nip9BlSpVVLduXU2ePFnnz5/3u+/o0aOaNWuWWrRooejoaK1evVqtW7dWXFycwsPDFRcXpzZt2mjnzp1XfA1JSkpKUlJSUrbxnN53enq6Bg0apMqVK/s+r86dO+vgwYN+xy1atEhJSUmKjo5WeHi4YmNj9dBDD+n06dMB4wFsRuK9gdStW1crVqzQ008/rRUrVigjI+OKx2/YsEE9e/bUc88950sMycnJ+uqrr3zHbNmyRYmJifrxxx81evRoffLJJ7rlllvUqVMnDRkyRNKFBNSgQQO/RLV69WodPXpUYWFhWrhwoW98wYIFqlWrlqKiogK+H6/Xq06dOmnSpEnKzMyUJM2bN0979uxR586dc/WZDBkyRP3791ebNm00Z84cTZ06VcnJyX6Jv3379po5c6b69eunefPmaezYsWrUqJEOHz4c8PnnzJmjMWPGaODAgfrnP/+pIkWKqGXLlkpLS/Md8+WXX+rBBx9UdHS0pk6dqiFDhmjKlCl+X16uJDk5Wb/++qvmzJnjN/7RRx/p7Nmzvqp4x44dqlSpkkaOHKm5c+fq9ddf1759+3Tbbbfp0KFDuXqtQLKystSiRQulpqaqbdu2mjNnjlJTUzV//nwlJSXpzJkzvliaNWumkJAQjR8/Xl9++aVSU1MVERGh9PT0axILkGcZ3DAOHTpk/vKXvxhJRpIJDg42iYmJZvDgwebEiRN+x5YpU8aEhYWZnTt3+sbOnDljihQpYh5//HHfWOvWrU1oaKjZtWuX3+ObNm1q8ufPb44ePWqMMWbs2LFGku+4QYMGmcqVK5v777/fdO7c2RhjTHp6uomIiDAvv/zyFd/H4sWLjSQzbdo0k5aWZjwej5k9e7Yxxpi//vWvJikpyRhjTLNmzUyZMmWyva+OHTv6fr7vvvtMQkLCFV8vMjLSPPvss1c8pmPHjtleS5IpXry4OX78uG9s//79xuv1msGDB/vGbrvtNlO6dGlz7tw539iJEydMdHS0yc1/0RMnTpjIyEhz//33+43XqlXLlC5d2mRmZub4uPPnz5uTJ0+aiIgIM2rUKN/4xc938eLFvrG77rrL3HXXXQHf95QpU4wk889//tPvuFWrVhlJ5s033zTGGDN9+nQjyaxfvz7g+wP+bKh4byDR0dH6+uuvtWrVKqWmpqpFixbaunWrevfurWrVqmWrehISEhQbG+v7OSwsTDfffLNfa3LRokVq2LChSpcu7ffYTp066fTp0/ruu+8kSY0aNZIkX9U7f/58NW7cWI0aNdL8+fMlSd99951OnTrlOzY3ypYtq6SkJI0fP16HDx/WrFmz1KVLl1w//vbbb9eGDRvUtWtXzZ07V8ePH8/xmPfff1+DBg3S8uXLA3YKLnX33XerQIECvp+LFy+uYsWK+T7DU6dOafXq1XrggQcUEhLiOy4yMlLNmzfP1WtERkbqkUce0eeff+5r22/cuFFr1qxRp06dfC33kydPqlevXqpQoYKCgoIUFBSkyMhInTp1Sps3b871e7qS2bNnKyoqSs2bN9f58+d9t4SEBP3Xf/2Xb6V0QkKCQkJC9Nhjj2nixIl+HQDgz47EewOqXbu2evXqpWnTpmnv3r167rnntGPHDl9r+KLo6Ohsjw0NDfW1C6UL88YlSpTIdlzJkiV990tSmTJlVL58eS1YsMCXkC8m3j179mjLli1asGCBwsPDlZiYeFXvJzk5WZ999plGjBih8PBwPfzww7l+bO/evTVs2DAtX75cTZs2VXR0tBo2bKjVq1f7jpk6dao6duyosWPHqm7duipSpIg6dOig/fv3B3z+QJ/hb7/95psD/72cxi4nOTlZ58+f1+TJkyVdWFTl8Xj8Wu5t27bVmDFj9Oijj2ru3LlauXKlVq1apaJFi/r9nf4RBw4c0NGjRxUSEqLg4GC/2/79+31f7i7+WyhWrJi6deum8uXLq3z58n5z68CfFYn3BhccHKyUlBRJF6qkqxUdHa19+/ZlG9+7d68kKSYmxjfWsGFDLVy4UEuXLlVWVpaSkpJUpUoVlSxZUvPnz9eCBQtUv359hYaGXlUMDz74oPLnz6/U1FS1bt1a4eHhuX5sUFCQevToobVr1+rIkSOaMmWKdu/erSZNmvgW+cTExGjkyJHasWOHdu7cqcGDB+uTTz65JucDFy5cWB6Px2+B2UW5SewXJSYmqkqVKpowYYIyMjL0wQcfqEGDBipbtqwk6dixY5o9e7ZefPFFvfTSS2rYsKFuu+02VatWTUeOHAn4/GFhYTp37ly28d93SWJiYhQdHa1Vq1bleHvzzTd9x9avX1+fffaZjh07puXLl6tu3bp69tln9fHHH+f6fQM2IvHeQHJKkJJ8bcaLVerVaNiwoRYtWuRLtBdNmjRJ+fPn99vcoVGjRjpw4IBGjhypOnXq+FqwDRs21IwZM7Rq1aqrajNfFB4ern79+ql58+Z68sknr/rxF0VFRenhhx9Wt27ddOTIEe3YsSPbMbGxserevbsaN26stWvX/sevdVFERIRq166tmTNn+i0qOnnyZI6rn6+kS5cu2rRpk/r06aODBw/6tdw9Ho+MMdm+1IwdO9a3MO1K4uLitHXrVr/ke/jwYX377bd+x9133306fPiwMjMzVbt27Wy3SpUqZXvufPny6Y477tA//vEPSbomnyuQl2U/rwR/Wk2aNNFNN92k5s2bq3LlysrKytL69es1fPhwRUZG6plnnrnq50xJSdHs2bN19913q1+/fipSpIg+/PBDzZkzR0OGDFGhQoV8x17cTWrevHkaMGCAb7xRo0bq2LGj78//iR49eqhHjx5X/bjmzZuratWqql27tooWLaqdO3dq5MiRKlOmjCpWrKhjx47p7rvvVtu2bVW5cmUVKFBAq1at8q1EvhYGDhyoZs2aqUmTJnrmmWeUmZmpoUOHKjIyMlfV6EUdOnTQyy+/rKFDhyoqKsovvoIFC+rOO+/U0KFDFRMTo7i4OC1dulTjxo3L1Qry9u3b65133lG7du30P//zPzp8+LCGDBmSbZOS1q1b68MPP9R///d/65lnntHtt9+u4OBg7dmzR4sXL1aLFi3UsmVLvf3221q0aJGaNWum2NhYnT171nda2H/6bwCwhturu+CcqVOnmrZt25qKFSuayMhIExwcbGJjY0379u3Npk2b/I4tU6aMadasWbbnyGl16w8//GCaN29uChUqZEJCQkz16tXNhAkTcoyhRo0aRpJZtmyZb+yXX34xkkx0dLTJysoK+D4uXdV8JblZ1Tx8+HCTmJhoYmJiTEhIiImNjTXJyclmx44dxhhjzp49a5544gkTHx9vChYsaMLDw02lSpVMSkqKOXXqlO95LrequVu3btni+n0MxhgzY8YMU61aNV8Mqamp5umnnzaFCxcO+HlcqmXLlkaS6dq1a7b79uzZYx566CFTuHBhU6BAAXPvvfeajRs3Zosnp1XNxhgzceJEU6VKFRMWFmZuueUWM3Xq1Bzfd0ZGhhk2bJipXr26CQsLM5GRkaZy5crm8ccfNz/99JMxxpjvvvvOtGzZ0pQpU8aEhoaa6Ohoc9ddd5lPP/30qt4vYCOPMca4mfgBZJeRkaGEhASVKlVK8+bNczscANcQrWYgD0hOTlbjxo1VokQJ7d+/X2+//bY2b97MKl/gT4jEC+QBJ06c0PPPP6+DBw8qODhYNWvW1Oeff858J/AnRKsZAAAHcToRAAC59NVXX6l58+YqWbKkPB6PZs6cedXPQeIFACCXTp06perVq2vMmDH/8XMwxwsAQC41bdpUTZs2/UPPQeIFANzQzp07l21L1NDQ0Kvevja3cp94Tx+7LgEAAP6E8hcKfMwf9ISnYOCDcuG/Unr47aYnXdiVr3///tfk+X+PihcAcEPr3bt3ti1nr1e1K5F4AQCWularg69nWzknJF4AgJW8Ho/bIfxHSLwAAOTSyZMn9fPPP/t+3r59u9avX68iRYooNjY2V8+R+52rWFwFAMgtBxZXPe29Nq8xOiv3+W3JkiW6++67s4137NhR77//fq6eg4oXAGAlrwud5qSkJP3RnZZJvAAAK9m69aKtcQMAYCUqXgCAlVjVDACAg2xt2doaNwAAVqLiBQBYyY1VzdcCiRcAYCVbW7a2xg0AgJWoeAEAVvKwqhkAAOfY2rK1NW4AAKxExQsAsBKrmgEAcJCtLVsSLwDASrZuGWnrFwYAAKxExQsAsJKtlSOJFwBgJVsXV9n6hQEAACtR8QIArGRr5UjiBQBYySs7e822fmEAAMBKVLwAACvZuriKxAsAsJKtLVsSLwDASrZWvLZ+YQAAwEpUvAAAK9m6qpnECwCwEq1mAAAQEBUvAMBKtlaOJF4AgJVoNQMAgICoeAEAVmJVMwAADqLVDAAAAqLiBQBYydKCl8QLALCTra1mEi8AwEq2Lq5ijhcAAAdR8QIArESrGQAAB9nasrU1bgAArETFCwCwkqWdZhIvAMBOXo+dqZdWMwAADqLiBQBYyc56l8QLALCUrYmXVjMAAA6i4gUAWMnWipfECwCwksfSVc0kXgCAlexMu8zxAgDgKCpeAICVbK0cSbwAACtZOsVr7RcGAACsRMULALCSx9LlVSReAICV7Ey7tJoBAHAUFS8AwEq2VrwkXgCAlbyWZl5azQAAOIiKFwBgJVY1AwDgIDvTLokXAGApdq4CAAABUfECAKxkacFL4gUA2Mlraeql1QwAgIOoeAEAVrKz3iXxAgAsxapmAAAQEBUvAMBKlha8JF4AgJ1s3TKSVjMAAA6i4gUAWMnWywKSeAEAVrI075J4AQB2sjXxMscLAICDqHgBAFaydVUziRcAYCV2rgIAAAFR8QIArGRr5UjiBQBYydJOs7VfGAAAsBIVLwDASh5LV1eReAEAVrIz7dJqBgDAUVS8AAAr2VrxkngBAFZijhcAAAfZellA5ngBAHAQFS8AwEoeS0teEi8AwEqWTvHSagYAwElUvAAAK9la8ZJ4AQBWsvV0IlrNAAA4iIoXAGAlSwteEi8AwE60mgEAQEBUvAAAK1la8JJ4AQB28lqaeUm8AAArWZp3meMFAMBJVLwAACvZuqqZxAsAsJLH0p6tpWEDAGAnKl4AgJVoNQMA4CBL8y6tZgAAnETFCwCwEq1mAAAcZGnepdUMAICTqHgBAFZir2YAABxkad4l8QIA7GTr4irmeAEAcBAVLwDASpYWvCReAICdbE28tJoBAHAQFS8AwEoer50lL4kXAGAlWs0AACAgKl4AgJXYuQoAAAdZmndpNQMA4CQqXgCAlWzdMpLECwCwkqV5l8QLALCTrRUvc7wAADiIihcAYCVLC14SLwDATrSaAQBAQFS8AAAreSwtHUm8AAAr0WoGAAABUfECAOzE9XgBAHAQrWYAABAIFS8AwEq2Lq4i8QIA7MQcLwAADrK04mWOFwAAB1HxAgCs5KHVDACAg2g1AwCAQKh4AQBWotUMAICTaDUDAIBAqHgBAHai1QwAgHNs3TKSVjMAAA6i4gUA2IlWMwAADrK01UziBQBYyWPpZKmlYQMAYCcqXgCAnWg1AwDgHFu3jKTVDACAg6h4AQB2otUMAICDaDUDAIBAqHgBAFayda9mEi8AwE60mgEAQCBUvAAAO9FqBgDAOczxAgDgJOZ4AQBAIFS8AAAr0WoGAMBJtJoBAEAgVLwAADvRagYAwDlcjxcAAARExQsAsBOtZgAAHESrGQAABELFCwCwEhtoAADgJEtbzSReAICdLK14meMFAMBBVLwAADtZWvGSeAEAdrI08dJqBgDAQVS8AAA7ee2sHUm8AAA70WoGAACBUPECAOxkacVL4gUA2MnSxEurGQAAB1HxAgDsxKpmAAAcZGmrmcQLALCTpYnXzjodAABLUfECAOxkacVL4gUA2MnSxVV2Rg0AgKWoeAEAdqLVDACAgyxNvLSaAQBwEBUvAMBOlla8JF4AgJU8rGoGAACBUPECAOxEqxkAAAeReAEAcJCliZc5XgAAHETFCwCwk6Wrmkm8AAA70WoGAACBUPECAOxkacVL4gUA2MnSxEurGQAAB1HxAgDsxKpmAAAcRKsZAAAEQsULALCTpRUviRcAYCfmeAEAcJClFa+dXxcAALAUFS8AwE6WVrwkXgCAnSxNvLSaAQBwEBUvAMBOrGoGAMBBtJoBAEAgVLwAADtZWvGSeAEAdvLY2bS1M2oAACxFxQsAsJOXVjMAAM6xtNVM4gUA2MnSxVV2fl0AAMBSVLwAADuxcxUAAA6i1QwAAAKh4gUA2IlVzQAAOIhWMwAACISKFwBgJ1Y1AwDgIFrNAAAgECpeAICdWNUMAICDuDoRAAAOsrTitTNqAAAsRcULALCTpauaSbwAADvRagYAAIFQ8QIA7MSqZgAAHGTpHC+tZgAAHETFCwCwk6WLq0i8AAA7MccLAICDLK147YwaAABLUfECAOxk6apmEi8AwE60mgEAQCBUvAAAO7GqGQAAB9FqBgAAgVDxAgDsxKpmAAAc5LWzaWtn1AAAWIqKFwBgJ1rNAAA4yNJVzSReAICdLK147fy6AACApah4AQB2snRVM4kXAGAnWs0AACAQKl4AgJ1Y1QwAgINoNQMAgECoeAEAdqLVDACAg7y0mgEAQABUvAAAO9FqBgDAQZauaibxAgDsZGnFa2fUAABYiooXAGAlD61mAAAcRKsZAAAEQsULALCTpRUviRcAYCd2rgIAAIFQ8QIA7ESrGQAAB1l6OpGdXxcAALAUFS8AwE60mgEAcJClrWYSLwDATpZWvHZGDQCApah4AQB2snQDDRIvAMBOtJoBAEAgVLwAADuxqhkAAAfRagYAAIFQ8QIA7ESrGQAAB9FqBgAAgVDxAgDs5LWzdiTxAgCs5GGOFwAABzHHCwAAAqHiBQDYiVYzAAAOotUMAAACoeIFANiJVjMAAA6y9DxeO6MGAMBSVLwAADvRagYAwEGsagYAAIFQ8QIA7ESrGQAAJ5F4AQBwjqUVL3O8AAA4iIoXAGAnSyteEi8AwFJ2Jl5azQAAOIiKFwBgJ1rNAAA4yM68S6sZAAAnUfECACxlZ8lL4gUA2MnSOV5azQAAOIiKFwBgJ0srXhIvAMBSJF4AAJxjacXLHC8AAA6i4gUAWMrOipfECwCwE61mAAAQCBUvAMBOlla8JF4AgKXsTLy0mgEAcBAVLwDASh5azQAAOMjSxEurGQAAB1HxAgAsZWfFS+IFANjJ0lYziRcAYCdLEy9zvAAAOIiKFwBgKTsrXhIvAMBOtJoBAEAgVLwAADvZWfCSeAEAtrIz89JqBgDAQVS8AAA7Wbq4isQLALCTpYmXVjMAAA6i4gUAWMrOipfECwCwE61mAAAc5PFcm9t/4M0331TZsmUVFhamWrVq6euvv871Y0m8AABchalTp+rZZ5/VK6+8onXr1ql+/fpq2rSpdu3alavHe4wxJldHnj72R+IEANxI8he6/q9x6ui1eZ6IqKs6/I477lDNmjX11ltv+caqVKmiBx54QIMHDw74eCpeAICdXGg1p6ena82aNbrnnnv8xu+55x59++23uXoOFlcBAG5o586d07lz5/zGQkNDFRoamu3YQ4cOKTMzU8WLF/cbL168uPbv35+7FzQuOHv2rElJSTFnz5514+UDysvx5eXYjCG+PyIvx2YM8f0ReTk2Y/J+fNdbSkqKkeR3S0lJyfHYX375xUgy3377rd/4oEGDTKVKlXL1ermf472Gjh8/rkKFCunYsWMqWLCg0y8fUF6OLy/HJhHfH5GXY5OI74/Iy7FJeT++6+1qKt709HTlz59f06ZNU8uWLX3jzzzzjNavX6+lS5cGfD3meAEAN7TQ0FAVLFjQ75ZT0pWkkJAQ1apVS/Pnz/cbnz9/vhITE3P1eszxAgBwFXr06KH27durdu3aqlu3rt59913t2rVLTzzxRK4eT+IFAOAqtGrVSocPH9bAgQO1b98+Va1aVZ9//rnKlCmTq8e7knhDQ0OVkpJy2VLebXk5vrwcm0R8f0Rejk0ivj8iL8cm5f348qKuXbuqa9eu/9FjXVlcBQDAjYrFVQAAOIjECwCAg0i8AAA4iMQLAICDSLx5XGZmppYuXarffvvN7VCAPGXBggWXve+dd95xMJKcderUSV999ZXbYVxWgwYNNGDAgGzjv/32mxo0aOBCRDcOxxJvgwYNdPTo0Wzjx48fd/0veeDAgTp9+nS28TNnzmjgwIEuRPRv+fLlU5MmTXL87PKKuLg4DRw4MNfXonTa0aNHNW/ePH3wwQeaNGmS3y0v2LZtm/r06aM2bdro119/lSR9+eWX+vHHH12OLG9r1qyZevbsqfT0dN/YwYMH1bx5c/Xu3dvFyC44ceKE7rnnHlWsWFGvvfaafvnlF7dD8rNkyRKNGTNGDzzwgE6dOuUbT09Pz9W2h/gD/sjG0lfD4/GYAwcOZBs/cOCACQoKciqMHHm93hxjO3TokPF6vS5E5K927dpmwYIFbodxWaNHjzY1a9Y0+fLlM40aNTJTpkzJM5utf/rpp6ZAgQLG6/WaQoUKmaioKN+tcOHCbodnlixZYsLDw02jRo1MSEiI2bZtmzHGmNdff9089NBDLkd3waRJk0xiYqIpUaKE2bFjhzHGmDfeeMPMnDnT1biWL19uKlasaOLj483GjRvN7NmzTbFixUxSUpLZtWuXq7FddOjQITNy5EiTkJBggoKCzL333mumTZtm0tPT3Q7NeDwes379enPHHXeYqlWrmu3btxtjjNm/f3+e+L33Z3bdE++GDRvMhg0bjMfjMYsXL/b9vGHDBrN27Vrz2muvmTJlylzvMK7I4/GYX3/9Ndv4woULTUxMjAsR+Zs7d65JSEgwn332mdm7d685duyY3y2vWL9+vXn66adN0aJFTeHChU23bt3MmjVrXI2pYsWK5plnnjGnTp1yNY7LqVOnjhk+fLgxxpjIyEhf4l25cqUpWbKkm6EZY4x58803TUxMjBk0aJAJDw/3xTdhwgSTlJTkcnTGnDx50rRr186Ehoaa4OBg8/rrr5usrCy3w8rR2rVrTffu3U1YWJiJiYkxzz77rNm6datr8Vwshs6ePWvatm1rYmJizOLFi0m8Drjuidfj8Riv12u8Xq/xeDzZbvnz5zfjxo273mHk6GLV4/V6fX++eCtYsKDxer2ma9eursR2qUs/r4uf5cXPMy/+B0lPTzcjR440oaGhxuv1mvj4eDNu3DhXfiHmz5/flyzyooiICJOWlmaM8U+827dvN6GhoW6GZowxpkqVKmbGjBnGGP/4fvjhBxMdHe1iZBesWbPGVKpUyZQvX96Eh4ebzp07m5MnT7odVjZ79+41qamp5uabbzYRERGmQ4cOpnHjxiYoKMiMGDHClZh+3+l79dVXTWhoqOnXr1+e/L3yZ3Ldt4zcvn27jDEqV66cVq5cqaJFi/ruCwkJUbFixZQvX77rHUaORo4cKWOMunTpogEDBqhQoUJ+scXFxalu3bquxHapxYsXux1CrmRkZGjGjBmaMGGC5s+frzp16ig5OVl79+7VK6+8ogULFuijjz5yNKYmTZpo9erVKleunKOvm1tRUVHat2+fypYt6ze+bt06lSpVyqWo/m379u2qUaNGtvHQ0FC/eUE3pKamKiUlRY899piGDh2qbdu2qV27doqPj9cHH3zg+v/djIwMffrpp5owYYLmzZun+Ph4Pffcc/rb3/6mAgUKSJI+/vhjPfnkk3ruueccj8/8btPCPn36qEqVKurYsaPjsdxornvivbhpdFZW1vV+qat28R9Y2bJllZiYqODgYJcjytldd93ldghXtHbtWk2YMEFTpkxRvnz51L59e73xxhuqXLmy75h77rlHd955pyPxfPrpp74/N2vWTC+88II2bdqkatWqZfs7vv/++x2J6XLatm2rXr16adq0afJ4PMrKytKyZcv0/PPPq0OHDq7GJl34v7F+/fpsm79/8cUXuuWWW1yK6oJRo0Zp5syZatq0qSTp1ltv1cqVK/Xyyy8rKSkp2/VVnVaiRAllZWWpTZs2WrlypRISErId06RJE0VFRTkem3ThS9WlhZAkPfTQQ6pcubJWr17tSkw3Csf3at60aZN27drltxJRcvcXYKDVuLGxsQ5FcnlHjx7VuHHjtHnzZnk8Ht1yyy3q0qWLX5Xulnz58qlx48ZKTk7WAw88kOMXmFOnTql79+6aMGHCdY/H683dYn2Px6PMzMzrHM2VZWRkqFOnTvr4449ljFFQUJAyMzPVtm1bvf/++651gy6aMGGC+vbtq+HDhys5OVljx47Vtm3bNHjwYI0dO1atW7d2LbZDhw4pJiYmx/uWLl3q+hfWyZMn669//avCwsJcjQN5j2OJNy0tTS1bttQPP/wgj8fja3N4PB5JcvUXoNfr9cWRE7d/Oa9evVpNmjRReHi4br/9dhljtHr1ap05c0bz5s1TzZo1XY1v586dub4cFnK2bds2rVu3TllZWapRo4YqVqzodkg+7733ngYNGqTdu3dLkkqVKqX+/fsrOTnZ5cgAOzmWeJs3b658+fLpvffe8833Hj58WD179tSwYcNUv359J8LI0YYNG/x+zsjI0Lp16zRixAj9/e9/14MPPuhSZBfUr19fFSpU0HvvvaegoAuzA+fPn9ejjz6qtLQ010/SL1eunFatWqXo6Gi/8aNHj6pmzZpKS0tzKTJp0qRJatWqVbbLnaWnp+vjjz/OE+1cWxw6dEhZWVkqVqyY26EAVnMs8cbExGjRokWKj49XoUKFtHLlSlWqVEmLFi1Sz549tW7dOifCuCpz5szR0KFDtWTJElfjCA8P17p16/zmTKULbfvatWvnuPmHk7xer/bv35/tF/KBAwcUGxvr6lxbvnz5tG/fvmyxHT58WMWKFXOlm9GjR49cHztixIjrGElg27dv1/nz57NV4D/99JOCg4MVFxfnTmCAxa774qqLMjMzFRkZKelCEt67d68qVaqkMmXKaMuWLU6FcVVuvvlmrVq1yu0wVLBgQe3atStb4t29e7dvdaQbLl3ENHfuXL/55szMTC1cuND1X8zGmBynEfbs2ePa/Hhuv2ReafrDKZ06dVKXLl2yJd4VK1Zo7Nixrn8pBWzkWOKtWrWqvv/+e5UrV0533HGHhgwZopCQEL377ruun+px/Phxv5+NMdq3b5/69++fJ+baWrVqpeTkZA0bNkyJiYnyeDz65ptv9MILL6hNmzauxfXAAw9IupAgfn8KwsVqaPjw4S5EJtWoUUMej0cej0cNGzb0teilC18Ktm/frnvvvdeV2Gw5PUy68CWhXr162cbr1Kmj7t27uxARYD/HEm+fPn185/0NGjRI9913n+rXr6/o6GhNnTrVqTByFBUVla26MMaodOnSmjJliktR/duwYcPk8XjUoUMHnT9/XtKFxPbkk08qNTXVtbguniJWtmxZrVq16rIrTN1w8UvB+vXr1aRJE1+3Rfr3OdoPPfSQS9HlbPfu3fJ4PLrpppvcDsXH4/HoxIkT2caPHTvm+qJDwFaOn050qSNHjqhw4cKut9R+vyG41+tV0aJFVaFCBb9KyW2nT5/Wtm3bZIxRhQoVlD9/frdDyvMmTpyoVq1a5dlTOs6fP68BAwZo9OjROnnypCQpMjJSTz31lFJSUlw/t/y+++5T/vz5fedoSxc6Bq1atdKpU6f0xRdfuBofYCNHEu/58+cVFham9evXq2rVqtf75a7a4MGDVbx4cXXp0sVvfPz48Tp48KB69erlUmR51+jRo/XYY48pLCxMo0ePvuKxTz/9tENRXd7q1at950BXqVJFtWrVcjskSdITTzyhGTNmaODAgb6dlr777jv1799fLVq00Ntvv+1qfJs2bdKdd96pqKgo35kHX3/9tY4fP65Fixblyf/PQF7nWMVbvnx5ffLJJ6pevboTL3dV4uLi9NFHHykxMdFvfMWKFWrdurW2b9/ueExXcwrTJ598ch0jyVnZsmW1evVqRUdHKy4u7rJdC4/H4+rpRL/88otat26tZcuW+XYIOnr0qBITEzVlyhSVLl3atdgkqVChQvr44499uy9d9MUXX6h169Y6duyYS5H92969ezVmzBht2LBB4eHhio+PV/fu3VWkSBG3QwOs5Ogcb+/evfXBBx/kuf+w+/fvV4kSJbKNFy1aVPv27XMhIuWJHamu5NIvIzt27HAvkAA6d+6sjIwMbd68WZUqVZIkbdmyRV26dFFycrLmzZvnanxhYWE5rvyOi4tTSEiI8wHloGTJknrttdfcDgP403Cs4q1Ro4Z+/vlnZWRkqEyZMoqIiPC7f+3atU6EkaOKFSsqJSVF7dq18xufPHmyUlJSXK3Y8rqMjAxVqlRJs2fPdn3v3pyEh4fr22+/zbbR/9q1a1WvXj2dOXPGpcguGDhwoP71r39pwoQJvk0+zp07p+TkZN+/S6d9//33qlq1qrxer77//vsrHhsfH+9QVMCfh2MV78VVpnnRo48+qmeffVYZGRlq0KCBJGnhwoV68cUX1bNnT5ej+7eDBw9qy5Yt8ng8uvnmm7NtcO6G4OBgnTt3zvUFcpcTGxurjIyMbOPnz5937eo/v59GWLBggW666SbfNMyGDRuUnp6uhg0buhGeEhISfBuiJCQk+G3xeqm8sNc1YCNXVzXnFcYYvfTSSxo9erTv4g1hYWHq1auX+vXr53J0Fy4w8NRTT2nSpEm+U3jy5cunDh066H//939dX92cmpqqf/3rXxo7dmyeWgUuSbNmzdJrr72mf/zjH6pVq5Y8Ho9Wr16tp556Sr169XLlC2Hnzp1zfawTF5X4vZ07dyo2NlYej0c7d+684rHs0Q1cPRLvJU6ePKnNmzcrPDxcFStWzLa/r1sef/xxLViwQGPGjPFtZvDNN9/o6aefVuPGjfXWW2+5Gl/Lli21cOFCRUZGqlq1atmmEdxY/HVR4cKFdfr0aZ0/f95vn+ugoKBscR45csSNEPOsjIwMPfbYY+rbt6/rm9wAfyaOJd7MzEy98cYb+r//+78cLwvIL73Li4mJ0fTp05WUlOQ3vnjxYj3yyCM6ePCgO4H9f4EqODeqtosmTpyY62O5AHh2UVFRWrt2LYkXuIYc6wsOGDBAY8eOVY8ePdS3b1+98sor2rFjh2bOnJkn2rl52enTp1W8ePFs48WKFXP9AgmSu4k1EBuS6fTp0y/7hdTNRYfShW7GzJkzr+rCDgCuLHdXDL8GPvzwQ7333nt6/vnnFRQUpDZt2mjs2LHq16+fli9f7lQYVqpbt65SUlJ09uxZ39iZM2c0YMAA36YLuLxt27apT58+atOmjX799VdJ0pdffqkff/zR5cgubETSuXNnFStWTOvWrdPtt9+u6OhopaWlZTu31w0VKlTQq6++qocffliDBw/W6NGj/W4Arp5jreaIiAht3rxZsbGxKlGihObMmeO7VmuNGjXyxEYBedUPP/ygpk2b6uzZs6pevbo8Ho/Wr1+v0NBQzZs3T7feeqvbIebZqm3p0qVq2rSp6tWrp6+++kqbN29WuXLlNGTIEK1cuVLTp093LTZJqly5slJSUtSmTRsVKFBAGzZsULly5dSvXz8dOXJEY8aMcTW+smXLXvY+tzdHAWzlWMV70003+TajqFChgm/jglWrVuWZRUx5VbVq1fTTTz9p8ODBSkhIUHx8vFJTU/Xzzz/niaSbl6u2l156SYMGDdL8+fP9NqS4++679d1337kY2QW7du3y7ZgWHh7uuyBB+/bt88QFOrZv3+67paWlKS0tze9nAP8B45BevXqZv//978YYY6ZNm2aCgoJMhQoVTEhIiOnVq5dTYVjptddeM+PGjcs2Pm7cOJOamupCRP4qVapkPvroI2OMMZGRkWbbtm3GGGP69u1runXr5mZoJiIiwqSlpRlj/GPbvn27CQ0NdTM0Y4wxZcuWNWvWrDHGGFO7dm3z9ttvG2OMmTt3rilcuLCbofmMHTvW3HrrrSYkJMSEhISYW2+91bz33ntuhwVYy7HE+3vLly83w4cPN7NmzXIrBGuUKVPGLFu2LNv48uXLTVxcnAsR+QsPDzc7duwwxhhTtGhRs379emOMMVu3bjVFihRxMzRTqlQp32d3aeL95JNPTLly5dwMzRhjTHJysunfv78xxpi33nrLhIeHm0aNGpmoqCjTpUsXl6Mzpk+fPiYiIsK89NJLZtasWWbWrFnmpZdeMpGRkeaVV15xOzzASo4l3rxeteVloaGhvqrtUtu2baNqC+CFF14wf/nLX8y+fftMgQIFzE8//WS++eYbU65cOV/Cc1NaWpo5d+6c7+epU6eap556yowaNcps3brVxcguiI6O9nUzLvXRRx+Z6OhoFyIC7OdY4s3rVVteVqFCBTN58uRs45MmTTJly5Z1ISJ/eblqS09PN23btjVer9d4PB4THBxsPB6PadeunTl//ryrsRljjNfrNQcOHMg2fujQIeP1el2IyF9UVFSOXwC2bNliChUq5HxAwJ+AY+fx5sUrANkir+8l/e677/q2snziiSdUpEgRffPNN2revLmeeOIJV2MLDg7Whx9+qFdffVVr165VVlaWatSooYoVK7oa10XmMicVnDx5UmFhYQ5Hk127du301ltvacSIEX7j7777rv72t7+5FBVgN8cSb+nSpbVs2bJspycsW7ZMJUuWdCoMK7344os6cuSIunbtmm0v6d69e7scneT1euX1/nuB/COPPKJHHnnEtXgCbfZw6Xnjv08oTrkYo8fjUb9+/fz2287MzNSKFSuUkJDgSmy/N27cOM2bN0916tSRdOHz2717tzp06OD3Wbv1WQK2cSzx5vWqLS/zeDx6/fXX1bdv3zyzl3Sgy8VdyulLx61bt87v5zVr1igzM9N3Pd6tW7cqX758qlWrlqNxXepijMYY/fDDD36nOoWEhKh69ep6/vnn3QrPZ+PGjapZs6akCxuRSBe6VEWLFtXGjRt9x+XVq1MBeZFjG2iYPH4FIFwdr9d72cvFXcrtS8eNGDFCS5Ys0cSJE1W4cGFJ0m+//abOnTurfv36rn/p69y5s0aNGqWCBQu6GgcA5zh+daK8egUgXJ1Al4u7lJuXjitVqlSOu3tt3LhR99xzj/bu3etSZABuVI5fPDUyMlK33Xab0y+La+zSZDp48GAVL15cXbp08Ttm/PjxOnjwoHr16uV0eD7Hjx/XgQMHsiXeX3/91bdLFAA4ybEtI/Hn9c4776hy5crZxm+99Va9/fbbLkT0by1btlTnzp01ffp07dmzR3v27NH06dOVnJysBx980NXYANyYHG81488nLCxMmzdvzrZiPS0tTbfccovfVZWcdvr0aT3//PMaP368MjIyJElBQUFKTk7W0KFDFRER4VpsAG5Mjrea8eeTl08Vy58/v958800NHTpU27ZtkzFGFSpUIOECcA2JF3+YDaeKRUREOH5aEwDkhFYz/jBOFQOA3CPx4prhVDEACIzECwCAgzidCAAAB5F4AQBwEIkXAAAHkXgBAHAQiRcAAAeReAEAcBCJFwAAB5F4AQBw0P8DP6ru9Z9K708AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "sns.heatmap(diamonds.isna(), ax=ax,\n",
    "           vmin=0, vmax=1, cmap=\"Reds\",\n",
    "           cbar_kws={\"ticks\":[0,1]})\n",
    "ax.set_yticks([])\n",
    "ax.set_title(\"Show Missing Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dummy variables\n",
    "\n",
    "Since in the diamond dataset we have three categorical input features, we need to convert them into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>cut_Ideal</th>\n",
       "      <th>cut_Premium</th>\n",
       "      <th>cut_Very Good</th>\n",
       "      <th>...</th>\n",
       "      <th>color_I</th>\n",
       "      <th>color_J</th>\n",
       "      <th>clarity_IF</th>\n",
       "      <th>clarity_VVS1</th>\n",
       "      <th>clarity_VVS2</th>\n",
       "      <th>clarity_VS1</th>\n",
       "      <th>clarity_VS2</th>\n",
       "      <th>clarity_SI1</th>\n",
       "      <th>clarity_SI2</th>\n",
       "      <th>clarity_I1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>0.91</td>\n",
       "      <td>61.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3911</td>\n",
       "      <td>6.19</td>\n",
       "      <td>6.24</td>\n",
       "      <td>3.82</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53684</th>\n",
       "      <td>0.81</td>\n",
       "      <td>64.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2714</td>\n",
       "      <td>5.84</td>\n",
       "      <td>5.88</td>\n",
       "      <td>3.75</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43174</th>\n",
       "      <td>0.54</td>\n",
       "      <td>59.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1389</td>\n",
       "      <td>5.40</td>\n",
       "      <td>5.35</td>\n",
       "      <td>3.17</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45253</th>\n",
       "      <td>0.51</td>\n",
       "      <td>61.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1660</td>\n",
       "      <td>5.13</td>\n",
       "      <td>5.15</td>\n",
       "      <td>3.17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15623</th>\n",
       "      <td>1.00</td>\n",
       "      <td>57.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6268</td>\n",
       "      <td>6.57</td>\n",
       "      <td>6.64</td>\n",
       "      <td>3.79</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  depth  table  price     x     y     z  cut_Ideal  cut_Premium  \\\n",
       "5811    0.91   61.5   60.0   3911  6.19  6.24  3.82      False         True   \n",
       "53684   0.81   64.0   60.0   2714  5.84  5.88  3.75      False        False   \n",
       "43174   0.54   59.0   60.0   1389  5.40  5.35  3.17      False         True   \n",
       "45253   0.51   61.7   56.0   1660  5.13  5.15  3.17       True        False   \n",
       "15623   1.00   57.4   58.0   6268  6.57  6.64  3.79      False        False   \n",
       "\n",
       "       cut_Very Good  ...  color_I  color_J  clarity_IF  clarity_VVS1  \\\n",
       "5811           False  ...    False    False       False         False   \n",
       "53684           True  ...     True    False       False         False   \n",
       "43174          False  ...    False    False       False         False   \n",
       "45253          False  ...    False    False       False         False   \n",
       "15623          False  ...    False    False       False         False   \n",
       "\n",
       "       clarity_VVS2  clarity_VS1  clarity_VS2  clarity_SI1  clarity_SI2  \\\n",
       "5811          False        False        False         True        False   \n",
       "53684         False        False         True        False        False   \n",
       "43174         False        False        False         True        False   \n",
       "45253         False        False         True        False        False   \n",
       "15623         False        False         True        False        False   \n",
       "\n",
       "       clarity_I1  \n",
       "5811        False  \n",
       "53684       False  \n",
       "43174       False  \n",
       "45253       False  \n",
       "15623       False  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = pd.get_dummies(diamonds)\n",
    "diamonds.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining inputs and output\n",
    "\n",
    "X = diamonds.drop(\"price\", axis=1)\n",
    "y = diamonds[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing training data\n",
    "\n",
    "st_scaler = StandardScaler()\n",
    "st_scaler.fit(X_train)\n",
    "X_train_scaled = st_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential(\n",
    "    [Dense(36, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    Dense(36, activation=\"relu\"),\n",
    "     Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "\n",
    "model.compile(loss='mse',\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"mae\", \"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">    Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)                │        <span style=\"color: #00af00; text-decoration-color: #00af00\">972</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,332</span> │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)                │        \u001b[38;5;34m972\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)                │      \u001b[38;5;34m1,332\u001b[0m │\n",
       "├─────────────────────────────────┼───────────────────────────┼────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │         \u001b[38;5;34m37\u001b[0m │\n",
       "└─────────────────────────────────┴───────────────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,341</span> (9.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,341\u001b[0m (9.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,341</span> (9.14 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,341\u001b[0m (9.14 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 26184266.0000 - mae: 3498.7603 - mse: 26184266.0000 - val_loss: 3408727.7500 - val_mae: 1260.8303 - val_mse: 3408727.7500\n",
      "Epoch 2/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2588535.5000 - mae: 1043.3481 - mse: 2588535.5000 - val_loss: 1045924.5000 - val_mae: 685.2427 - val_mse: 1045924.5000\n",
      "Epoch 3/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1293378.5000 - mae: 667.8047 - mse: 1293378.5000 - val_loss: 811358.9375 - val_mae: 588.1666 - val_mse: 811358.9375\n",
      "Epoch 4/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1100735.2500 - mae: 594.1044 - mse: 1100735.2500 - val_loss: 749241.6875 - val_mae: 565.3237 - val_mse: 749241.6875\n",
      "Epoch 5/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1200850.3750 - mae: 578.2057 - mse: 1200850.3750 - val_loss: 713823.1875 - val_mae: 541.7033 - val_mse: 713823.1875\n",
      "Epoch 6/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 925381.5625 - mae: 557.4418 - mse: 925381.5625 - val_loss: 686769.3125 - val_mae: 521.0872 - val_mse: 686769.3125\n",
      "Epoch 7/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 864633.3125 - mae: 528.0114 - mse: 864633.3125 - val_loss: 668291.9375 - val_mae: 502.7083 - val_mse: 668291.9375\n",
      "Epoch 8/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 737170.4375 - mae: 502.4736 - mse: 737170.4375 - val_loss: 673636.4375 - val_mae: 489.9476 - val_mse: 673636.4375\n",
      "Epoch 9/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 761878.5625 - mae: 481.3173 - mse: 761878.5625 - val_loss: 604028.6875 - val_mae: 463.4848 - val_mse: 604028.6875\n",
      "Epoch 10/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 745094.4375 - mae: 471.8315 - mse: 745094.4375 - val_loss: 585851.3750 - val_mae: 446.0710 - val_mse: 585851.3750\n",
      "Epoch 11/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 694798.5625 - mae: 448.9347 - mse: 694798.5625 - val_loss: 570548.2500 - val_mae: 431.4740 - val_mse: 570548.2500\n",
      "Epoch 12/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 632692.8750 - mae: 440.4065 - mse: 632692.8750 - val_loss: 549965.3125 - val_mae: 419.9339 - val_mse: 549965.3125\n",
      "Epoch 13/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 618170.9375 - mae: 421.6387 - mse: 618170.9375 - val_loss: 546186.8125 - val_mae: 411.3351 - val_mse: 546186.8125\n",
      "Epoch 14/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 622803.6875 - mae: 411.4966 - mse: 622803.6875 - val_loss: 522536.0000 - val_mae: 401.0435 - val_mse: 522536.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 795427.4375 - mae: 411.9287 - mse: 795427.4375 - val_loss: 513967.9062 - val_mae: 395.9600 - val_mse: 513967.9062\n",
      "Epoch 16/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 685188.6250 - mae: 401.2537 - mse: 685188.6250 - val_loss: 498711.1562 - val_mae: 389.0706 - val_mse: 498711.1562\n",
      "Epoch 17/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 587253.7500 - mae: 394.9258 - mse: 587253.7500 - val_loss: 492183.5938 - val_mae: 383.7791 - val_mse: 492183.5938\n",
      "Epoch 18/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 625385.6875 - mae: 389.0536 - mse: 625385.6875 - val_loss: 480827.8438 - val_mae: 377.5490 - val_mse: 480827.8438\n",
      "Epoch 19/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 578283.0000 - mae: 377.0843 - mse: 578283.0000 - val_loss: 474425.4375 - val_mae: 375.8075 - val_mse: 474425.4375\n",
      "Epoch 20/20\n",
      "\u001b[1m944/944\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 513790.6875 - mae: 370.9321 - mse: 513790.6875 - val_loss: 468257.7812 - val_mae: 372.2556 - val_mse: 468257.7812\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train.values,\n",
    "                   epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 757.6409 ],\n",
       "       [ 780.889  ],\n",
       "       [ 831.60333],\n",
       "       [3875.4863 ],\n",
       "       [6227.572  ],\n",
       "       [2141.3782 ],\n",
       "       [ 641.4014 ],\n",
       "       [ 807.64264],\n",
       "       [ 676.93097],\n",
       "       [6684.345  ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train_scaled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.745873e+07</td>\n",
       "      <td>2698.574219</td>\n",
       "      <td>1.745873e+07</td>\n",
       "      <td>3.408728e+06</td>\n",
       "      <td>1260.830322</td>\n",
       "      <td>3.408728e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.035282e+06</td>\n",
       "      <td>901.082153</td>\n",
       "      <td>2.035282e+06</td>\n",
       "      <td>1.045924e+06</td>\n",
       "      <td>685.242676</td>\n",
       "      <td>1.045924e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.223987e+06</td>\n",
       "      <td>635.504639</td>\n",
       "      <td>1.223987e+06</td>\n",
       "      <td>8.113589e+05</td>\n",
       "      <td>588.166626</td>\n",
       "      <td>8.113589e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.101090e+06</td>\n",
       "      <td>590.578369</td>\n",
       "      <td>1.101090e+06</td>\n",
       "      <td>7.492417e+05</td>\n",
       "      <td>565.323730</td>\n",
       "      <td>7.492417e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.038076e+06</td>\n",
       "      <td>568.154053</td>\n",
       "      <td>1.038076e+06</td>\n",
       "      <td>7.138232e+05</td>\n",
       "      <td>541.703308</td>\n",
       "      <td>7.138232e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.863046e+05</td>\n",
       "      <td>546.661255</td>\n",
       "      <td>9.863046e+05</td>\n",
       "      <td>6.867693e+05</td>\n",
       "      <td>521.087158</td>\n",
       "      <td>6.867693e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9.368318e+05</td>\n",
       "      <td>523.236633</td>\n",
       "      <td>9.368318e+05</td>\n",
       "      <td>6.682919e+05</td>\n",
       "      <td>502.708313</td>\n",
       "      <td>6.682919e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.898567e+05</td>\n",
       "      <td>503.232330</td>\n",
       "      <td>8.898567e+05</td>\n",
       "      <td>6.736364e+05</td>\n",
       "      <td>489.947632</td>\n",
       "      <td>6.736364e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.497239e+05</td>\n",
       "      <td>481.840607</td>\n",
       "      <td>8.497239e+05</td>\n",
       "      <td>6.040287e+05</td>\n",
       "      <td>463.484772</td>\n",
       "      <td>6.040287e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.131624e+05</td>\n",
       "      <td>466.495148</td>\n",
       "      <td>8.131624e+05</td>\n",
       "      <td>5.858514e+05</td>\n",
       "      <td>446.071045</td>\n",
       "      <td>5.858514e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.766858e+05</td>\n",
       "      <td>450.081177</td>\n",
       "      <td>7.766858e+05</td>\n",
       "      <td>5.705482e+05</td>\n",
       "      <td>431.473969</td>\n",
       "      <td>5.705482e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7.462061e+05</td>\n",
       "      <td>435.037872</td>\n",
       "      <td>7.462061e+05</td>\n",
       "      <td>5.499653e+05</td>\n",
       "      <td>419.933899</td>\n",
       "      <td>5.499653e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.170344e+05</td>\n",
       "      <td>423.993103</td>\n",
       "      <td>7.170344e+05</td>\n",
       "      <td>5.461868e+05</td>\n",
       "      <td>411.335083</td>\n",
       "      <td>5.461868e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.916581e+05</td>\n",
       "      <td>413.955444</td>\n",
       "      <td>6.916581e+05</td>\n",
       "      <td>5.225360e+05</td>\n",
       "      <td>401.043518</td>\n",
       "      <td>5.225360e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.679937e+05</td>\n",
       "      <td>406.012024</td>\n",
       "      <td>6.679937e+05</td>\n",
       "      <td>5.139679e+05</td>\n",
       "      <td>395.960022</td>\n",
       "      <td>5.139679e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.485871e+05</td>\n",
       "      <td>398.794159</td>\n",
       "      <td>6.485871e+05</td>\n",
       "      <td>4.987112e+05</td>\n",
       "      <td>389.070648</td>\n",
       "      <td>4.987112e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.275487e+05</td>\n",
       "      <td>392.127686</td>\n",
       "      <td>6.275487e+05</td>\n",
       "      <td>4.921836e+05</td>\n",
       "      <td>383.779083</td>\n",
       "      <td>4.921836e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.088593e+05</td>\n",
       "      <td>387.304291</td>\n",
       "      <td>6.088593e+05</td>\n",
       "      <td>4.808278e+05</td>\n",
       "      <td>377.549042</td>\n",
       "      <td>4.808278e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.925859e+05</td>\n",
       "      <td>381.872437</td>\n",
       "      <td>5.925859e+05</td>\n",
       "      <td>4.744254e+05</td>\n",
       "      <td>375.807495</td>\n",
       "      <td>4.744254e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.766081e+05</td>\n",
       "      <td>377.464417</td>\n",
       "      <td>5.766081e+05</td>\n",
       "      <td>4.682578e+05</td>\n",
       "      <td>372.255615</td>\n",
       "      <td>4.682578e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            loss          mae           mse      val_loss      val_mae  \\\n",
       "0   1.745873e+07  2698.574219  1.745873e+07  3.408728e+06  1260.830322   \n",
       "1   2.035282e+06   901.082153  2.035282e+06  1.045924e+06   685.242676   \n",
       "2   1.223987e+06   635.504639  1.223987e+06  8.113589e+05   588.166626   \n",
       "3   1.101090e+06   590.578369  1.101090e+06  7.492417e+05   565.323730   \n",
       "4   1.038076e+06   568.154053  1.038076e+06  7.138232e+05   541.703308   \n",
       "5   9.863046e+05   546.661255  9.863046e+05  6.867693e+05   521.087158   \n",
       "6   9.368318e+05   523.236633  9.368318e+05  6.682919e+05   502.708313   \n",
       "7   8.898567e+05   503.232330  8.898567e+05  6.736364e+05   489.947632   \n",
       "8   8.497239e+05   481.840607  8.497239e+05  6.040287e+05   463.484772   \n",
       "9   8.131624e+05   466.495148  8.131624e+05  5.858514e+05   446.071045   \n",
       "10  7.766858e+05   450.081177  7.766858e+05  5.705482e+05   431.473969   \n",
       "11  7.462061e+05   435.037872  7.462061e+05  5.499653e+05   419.933899   \n",
       "12  7.170344e+05   423.993103  7.170344e+05  5.461868e+05   411.335083   \n",
       "13  6.916581e+05   413.955444  6.916581e+05  5.225360e+05   401.043518   \n",
       "14  6.679937e+05   406.012024  6.679937e+05  5.139679e+05   395.960022   \n",
       "15  6.485871e+05   398.794159  6.485871e+05  4.987112e+05   389.070648   \n",
       "16  6.275487e+05   392.127686  6.275487e+05  4.921836e+05   383.779083   \n",
       "17  6.088593e+05   387.304291  6.088593e+05  4.808278e+05   377.549042   \n",
       "18  5.925859e+05   381.872437  5.925859e+05  4.744254e+05   375.807495   \n",
       "19  5.766081e+05   377.464417  5.766081e+05  4.682578e+05   372.255615   \n",
       "\n",
       "         val_mse  \n",
       "0   3.408728e+06  \n",
       "1   1.045924e+06  \n",
       "2   8.113589e+05  \n",
       "3   7.492417e+05  \n",
       "4   7.138232e+05  \n",
       "5   6.867693e+05  \n",
       "6   6.682919e+05  \n",
       "7   6.736364e+05  \n",
       "8   6.040287e+05  \n",
       "9   5.858514e+05  \n",
       "10  5.705482e+05  \n",
       "11  5.499653e+05  \n",
       "12  5.461868e+05  \n",
       "13  5.225360e+05  \n",
       "14  5.139679e+05  \n",
       "15  4.987112e+05  \n",
       "16  4.921836e+05  \n",
       "17  4.808278e+05  \n",
       "18  4.744254e+05  \n",
       "19  4.682578e+05  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>val_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4178.364273</td>\n",
       "      <td>1846.274018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426.633275</td>\n",
       "      <td>1022.704503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1106.339403</td>\n",
       "      <td>900.754649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049.328297</td>\n",
       "      <td>865.587481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1018.860209</td>\n",
       "      <td>844.880576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>993.128674</td>\n",
       "      <td>828.715459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>967.900725</td>\n",
       "      <td>817.491246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>943.322155</td>\n",
       "      <td>820.753579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>921.804718</td>\n",
       "      <td>777.192825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901.755164</td>\n",
       "      <td>765.409286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>881.297800</td>\n",
       "      <td>755.346444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>863.832232</td>\n",
       "      <td>741.596462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>846.778823</td>\n",
       "      <td>739.044527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>831.659824</td>\n",
       "      <td>722.866516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>817.308808</td>\n",
       "      <td>716.915550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>805.349070</td>\n",
       "      <td>706.194843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>792.179707</td>\n",
       "      <td>701.557976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>780.294376</td>\n",
       "      <td>693.417510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>769.795996</td>\n",
       "      <td>688.785480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>759.347129</td>\n",
       "      <td>684.293637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rmse     val_rmse\n",
       "0   4178.364273  1846.274018\n",
       "1   1426.633275  1022.704503\n",
       "2   1106.339403   900.754649\n",
       "3   1049.328297   865.587481\n",
       "4   1018.860209   844.880576\n",
       "5    993.128674   828.715459\n",
       "6    967.900725   817.491246\n",
       "7    943.322155   820.753579\n",
       "8    921.804718   777.192825\n",
       "9    901.755164   765.409286\n",
       "10   881.297800   755.346444\n",
       "11   863.832232   741.596462\n",
       "12   846.778823   739.044527\n",
       "13   831.659824   722.866516\n",
       "14   817.308808   716.915550\n",
       "15   805.349070   706.194843\n",
       "16   792.179707   701.557976\n",
       "17   780.294376   693.417510\n",
       "18   769.795996   688.785480\n",
       "19   759.347129   684.293637"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_metrics_df = history_df[[\"mse\", \"val_mse\"]].apply(np.sqrt)\n",
    "root_metrics_df.rename({\"mse\":\"rmse\", \"val_mse\":\"val_rmse\"}, axis=1, inplace=True)\n",
    "root_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure(figsize=(14,6), dpi=100)\n",
    "\n",
    "plt.plot(root_metrics_df[\"rmse\"], label = 'Training error')\n",
    "plt.plot(root_metrics_df[\"val_rmse\"], label = 'Validation error')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Root Mean Squared Error\")\n",
    "\n",
    "# plt.xlim([0, epochs])\n",
    "plt.xticks(range(1,20))\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "X_test_scaled = st_scaler.transform(X_test)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  690.3983012179995\n",
      "MAE:  375.92524187698336\n"
     ]
    }
   ],
   "source": [
    "# Report regression performance on test set\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "print(\"RMSE: \", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"MAE: \", mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
